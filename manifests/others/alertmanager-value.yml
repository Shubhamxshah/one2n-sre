global:
  resolve_timeout: 5m

route:
  receiver: 'slack-notifications'
  group_by: ['alertname']  # Simplified grouping
  group_wait: 10s          # Reduced for faster initial notification
  group_interval: 30s      # Reduced for testing
  repeat_interval: 1h      # Reduced from 12h
  routes:
  - receiver: 'slack-notifications'
    matchers:
    - severity=~"warning|critical"
    continue: true

inhibit_rules:
- equal:
  - alertname
  - namespace
  source_matchers:
  - severity="critical"
  target_matchers:
  - severity="warning"

receivers:
- name: 'slack-notifications'
  slack_configs:
  - channel: '#devops'
    send_resolved: true
    icon_emoji: ':warning:'
    # Added more context to the messages
    title: >-
      [{{ .Status | toUpper }}] {{ .GroupLabels.alertname }} - {{ len .Alerts }} alert(s)
    text: >-
      {{- range .Alerts }}
      *Alert:* {{ .Annotations.summary }}
      *Description:* {{ .Annotations.description }}
      *Severity:* {{ .Labels.severity }}
      *Value:* {{ .Labels.value }}
      *Started:* {{ .StartsAt.Format "2006-01-02 15:04:05" }}
      {{- if ne .Status "firing" }}
      *Resolved:* {{ .EndsAt.Format "2006-01-02 15:04:05" }}
      {{- end }}
      {{ end }}
prometheus:
  additionalPrometheusRulesMap:
    production-alerts:
      groups:
      # Resource Utilization Alerts
      - name: resource_alerts
        rules:
        - alert: HighCPUUsage
          expr: |
            avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[5m]) * 100) < 20
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "High CPU usage detected"
            description: "CPU usage is above 80% on {{ $labels.instance }} for 5 minutes"

        - alert: HighDiskUsage
          expr: |
            (node_filesystem_avail_bytes * 100) / node_filesystem_size_bytes < 20
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "High disk usage detected"
            description: "Disk usage is above 80% on {{ $labels.instance }} ({{ $labels.mountpoint }})"

      # Application Performance Alerts
      - name: application_alerts
        rules:
        - alert: HighErrorRate
          expr: |
            sum(rate(http_requests_total{status=~"5.."}[10m])) / sum(rate(http_requests_total[10m])) * 100 > 5
          for: 2m
          labels:
            severity: critical
          annotations:
            summary: "High error rate detected"
            description: "Error rate is above 5% in the last 10 minutes. Current value: {{ $value }}%"

        - alert: HighP90Latency
          expr: |
            histogram_quantile(0.90, sum by(le) (rate(http_request_duration_seconds_bucket[5m]))) > 1
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "High P90 latency detected"
            description: "P90 latency is above 1 second. Current value: {{ $value }}s"

        - alert: HighP99Latency
          expr: |
            histogram_quantile(0.99, sum by(le) (rate(http_request_duration_seconds_bucket[5m]))) > 5
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "High P99 latency detected"
            description: "P99 latency is above 5 seconds. Current value: {{ $value }}s"

      # Infrastructure Components Alerts (using Blackbox metrics)
      - name: infrastructure_alerts
        rules:
        - alert: VaultServerDown
          expr: |
            probe_success{instance="vault.vault.svc.cluster.local:8200"} == 0
          for: 1m
          labels:
            severity: critical
          annotations:
            summary: "Vault server is down"
            description: "Vault server is not responding to HTTP checks"

        - alert: ArgoCDServerDown
          expr: |
            probe_success{instance="argocd-metrics.argocd:8082/metrics"} == 0
          for: 1m
          labels:
            severity: critical
          annotations:
            summary: "ArgoCD server is down"
            description: "ArgoCD server is not responding to HTTP checks"

        - alert: DatabaseDown
          expr: |
            pg_up == 0
          for: 1m
          labels:
            severity: critical
          annotations:
            summary: "Database is down"
            description: "PostgreSQL database is not responding"
